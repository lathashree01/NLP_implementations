{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "492ff3e7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f935b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e74abb",
   "metadata": {},
   "source": [
    "## Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b2b83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    # Do dot product of input and weights\n",
    "\tQ = X @ W_q\n",
    "\tK = X @ W_k\n",
    "\tV = X @ W_v\n",
    "\treturn Q, K, V\n",
    "\n",
    "def softmax(X):\n",
    "    # Doing this make the softmax numerically stable\n",
    "    X_shifted = X - np.max(X, axis=1, keepdims=True)\n",
    "    return np.exp(X_shifted)/np.sum(np.exp(X_shifted), axis=1, keepdims=True)\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    d_k = Q.shape[1]\n",
    "    scores = (Q @ K.T) / np.sqrt(d_k)\n",
    "    attention_weights = softmax(scores)\n",
    "    attention_output = np.matmul(attention_weights, V)\n",
    "    return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d70bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0], [0, 1]])\n",
    "W_q = np.array([[1, 0], [0, 1]])\n",
    "W_k = np.array([[1, 0], [0, 1]])\n",
    "W_v = np.array([[1, 2], [3, 4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04d8ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v)\n",
    "output = self_attention(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb52472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6604769 2.6604769]\n",
      " [2.3395231 3.3395231]]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01552d80",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "899684f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_qkv(X, W_q, W_k, W_v):\n",
    "    Q = X @ W_q\n",
    "    K = X @ W_k\n",
    "    V = X @ W_v\n",
    "    return Q, K, V\n",
    "\n",
    "def softmax(X):\n",
    "    X_shifted = X - np.max(X, axis=1, keepdims=True)\n",
    "    return np.exp(X_shifted) / np.sum(np.exp(X_shifted), axis=1, keepdims=True)\n",
    "\n",
    "def self_attention(Q, K, V):\n",
    "    dk = K.shape[1]\n",
    "    scores = (Q @ K.T)/ np.sqrt(dk)\n",
    "    attn_wei = softmax(scores)\n",
    "    attn_outputs = attn_wei @ V\n",
    "    return attn_outputs\n",
    "\n",
    "\n",
    "def multi_head_attention(Q, K, V, n_heads):\n",
    "    seq_len, d_model = X.shape\n",
    "    # Need to ensure the dimension can be divided equally for all heads\n",
    "    assert d_model % n_heads == 0\n",
    "\n",
    "    d_k = d_model // n_heads\n",
    "\n",
    "    # perform splits over heads\n",
    "    Q_reshaped = Q.reshape(seq_len, n_heads, d_k).transpose(1,0,2)\n",
    "    K_reshaped = K.reshape(seq_len, n_heads, d_k).transpose(1,0,2)\n",
    "    V_reshaped = V.reshape(seq_len, n_heads, d_k).transpose(1,0,2)\n",
    "\n",
    "    attention = []\n",
    "\n",
    "    # calculate attention\n",
    "    for i in range(n_heads):\n",
    "        attn = self_attention(Q_reshaped[i],K_reshaped[i], V_reshaped[i])\n",
    "        attention.append(attn)\n",
    "    \n",
    "    # concatenate the attention output\n",
    "    attention_op = np.concatenate(attention, axis=-1)\n",
    "    return attention_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b04986ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[103. 109.  46.  99.]\n",
      " [103. 109.  46.  99.]\n",
      " [103. 109.  46.  99.]\n",
      " [103. 109.  46.  99.]]\n"
     ]
    }
   ],
   "source": [
    "m, n = 4, 4 \n",
    "n_heads = 2 \n",
    "np.random.seed(42) \n",
    "X = np.arange(m*n).reshape(m,n) \n",
    "X = np.random.permutation(X.flatten()).reshape(m, n) \n",
    "\n",
    "W_q = np.random.randint(0,4,size=(n,n)) \n",
    "W_k = np.random.randint(0,5,size=(n,n)) \n",
    "W_v = np.random.randint(0,6,size=(n,n)) \n",
    "\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v) \n",
    "print(multi_head_attention(Q, K, V, n_heads))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3165ee8a",
   "metadata": {},
   "source": [
    "## Masked multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a86999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_qkv(X: np.ndarray, W_q: np.ndarray, W_k: np.ndarray, W_v: np.ndarray):\n",
    "\t\"\"\"\n",
    "\tCompute Query (Q), Key (K), and Value (V) matrices.\n",
    "\t\"\"\"\n",
    "\treturn np.dot(X, W_q), np.dot(X, W_k), np.dot(X, W_v)\n",
    "\n",
    "def softmax(X):\n",
    "    X = X - np.max(X, axis=1, keepdims=True)\n",
    "    return np.exp(X) / np.sum(np.exp(X), axis=1, keepdims=True)\n",
    "\n",
    "def masked_attention(Q: np.ndarray, K: np.ndarray, V: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute masked self-attention.\n",
    "    \"\"\"\n",
    "    dk = K.shape[1]\n",
    "    QKt = Q @ K.T\n",
    "    scores = np.divide(QKt, np.sqrt(dk))\n",
    "    masked_scores = np.add(scores, mask)\n",
    "    softmax_scores = softmax(masked_scores)\n",
    "    attention = softmax_scores @ V\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06d5eb0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf, -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf, -inf, -inf],\n",
       "       [  0.,   0.,   0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42) \n",
    "X = np.arange(48).reshape(6,8) \n",
    "X = np.random.permutation(X.flatten()).reshape(6, 8)\n",
    "\n",
    "mask = np.triu(np.ones((6, 6))*(-np.inf), k=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5dab841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[547. 490. 399. 495. 485. 439. 645. 393.]\n",
      " [547. 490. 399. 495. 485. 439. 645. 393.]\n",
      " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
      " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
      " [471. 472. 429. 538. 377. 450. 531. 362.]\n",
      " [471. 472. 429. 538. 377. 450. 531. 362.]]\n"
     ]
    }
   ],
   "source": [
    "W_q = np.random.randint(0,4,size=(8,8)) \n",
    "W_k = np.random.randint(0,5,size=(8,8)) \n",
    "W_v = np.random.randint(0,6,size=(8,8)) \n",
    "\n",
    "Q, K, V = compute_qkv(X, W_q, W_k, W_v) \n",
    "print(masked_attention(Q, K, V, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c87f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
